import asyncio
import time
import pandas as pd
from openai import AsyncOpenAI
import json
import argparse
from tqdm import tqdm
import aiofiles
import os

# NOTE: this script is mostly cursor-written and not well tested. There is no "evaluation" here
# we just made sure we can load our dataset and run example rollouts against our eval server 

async def send_request(client, messages, request_id, extra_info=None, max_tokens=30000):
    """Send a single request and measure the time it takes"""
    start_time = time.time()
    
    try:
        completion = await client.chat.completions.create(
            model="Qwen/Qwen2.5-Coder-7B-Instruct",  # This doesn't matter - the eval server uses whatever model is loaded
            messages=messages,
            temperature=0.6,
            max_tokens=max_tokens,
            top_p=0.95,
            n=1,
            extra_body={"extra_infos": [extra_info]} if extra_info else None
        )
        
        end_time = time.time()
        
        # Extract the full conversation history if available
        conversation_history = getattr(completion, "conversation_history", None)
        
        return {
            "request_id": request_id,
            "duration": end_time - start_time,
            "response": completion.choices[0].message.content,
            "success": True,
            "conversation_history": conversation_history,
            "finish_reason": completion.choices[0].finish_reason,
            "extra_info": extra_info
        }
    except Exception as e:
        end_time = time.time()
        return {
            "request_id": request_id,
            "duration": end_time - start_time,
            "error": str(e),
            "success": False,
            "extra_info": extra_info
        }

async def evaluate_dataset(dataset_path, output_dir, base_url="http://0.0.0.0:5000"):
    """Evaluate an entire dataset of vulnerability POCs"""
    
    # Load the dataset
    df = pd.read_parquet(dataset_path).head(1)
    print(f"Loaded dataset with {len(df)} examples")
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Initialize the OpenAI client
    client = AsyncOpenAI(api_key="dummy-key", base_url=base_url)
    
    # Process each example sequentially
    results = []
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing examples"):
        messages = row['prompt']  # Assuming this is already in the correct message format
        extra_info = row['extra_info']  # extra info is required (contains repository info)
        result = await send_request(client, messages, idx, extra_info)
        results.append(result)
    
    # Calculate statistics
    successful_requests = [r for r in results if r["success"]]
    failed_requests = [r for r in results if not r["success"]]
    
    # TODO: !!!!!!!!!!!!!
    # SUCCESS HERE JUST MEANS THERE WASN'T AN ERROR! WE HAVE YET TO EVALUATE ANYTHING
    # TODO: !!!!!!!!!!!!!
    if successful_requests:
        avg_request_time = sum(r["duration"] for r in successful_requests) / len(successful_requests)
    else:
        avg_request_time = 0
    
    # Generate summary
    summary = {
        "total_examples": len(df),
        "successful_requests": len(successful_requests),
        "failed_requests": len(failed_requests),
        "average_request_time": avg_request_time,
        "total_duration": sum(r["duration"] for r in results),
        "finish_reasons": {
            r["finish_reason"]: len([x for x in successful_requests if x["finish_reason"] == r["finish_reason"]])
            for r in successful_requests if "finish_reason" in r
        }
    }
    
    # Save final results and summary
    final_results_path = os.path.join(output_dir, "final_results.json")
    summary_path = os.path.join(output_dir, "summary.json")
    
    async with aiofiles.open(final_results_path, 'w') as f:
        await f.write(json.dumps(results, indent=2))
    
    async with aiofiles.open(summary_path, 'w') as f:
        await f.write(json.dumps(summary, indent=2))
    
    print("\n===== EVALUATION SUMMARY =====")
    print(f"Total examples: {len(df)}")
    print(f"Successful requests: {len(successful_requests)}")
    print(f"Failed requests: {len(failed_requests)}")
    print(f"Average request time: {avg_request_time:.2f} seconds")
    print(f"Total duration: {summary['total_duration']:.2f} seconds")
    print("\nFinish reasons distribution:")
    for reason, count in summary["finish_reasons"].items():
        print(f"  {reason}: {count}")
    print(f"\nResults saved to: {output_dir}")
    
    if failed_requests:
        print("\nFailed requests:")
        for req in failed_requests[:5]:  # Show first 5 failures
            print(f"  Request {req['request_id']}: {req['error']}")
        if len(failed_requests) > 5:
            print(f"  ... and {len(failed_requests) - 5} more failures")

def main(debug=False):
    if debug:
        asyncio.run(evaluate_dataset(
            dataset_path="/home/johnheyer/data/vulnerability_poc_execution/oracle_hypotheses_o4_mini.parquet",
            output_dir="/home/johnheyer/eval_results/vulnerability_poc_execution/oracle_hypotheses_o4_mini_results",
            base_url="http://0.0.0.0:8000"
        ))
        return
    parser = argparse.ArgumentParser(description="Evaluate a vulnerability POC dataset using the eval service")
    parser.add_argument("--dataset_path", type=str, required=True, help="Path to the parquet dataset")
    parser.add_argument("--output_dir", type=str, required=True, help="Directory to save results")
    parser.add_argument("--base_url", type=str, default="http://0.0.0.0:5000", help="Base URL of the eval service")
    
    args = parser.parse_args()
    asyncio.run(evaluate_dataset(
            dataset_path=args.dataset_path,
            output_dir=args.output_dir,
            base_url=args.base_url
        ))

if __name__ == "__main__":
    main(debug=True)